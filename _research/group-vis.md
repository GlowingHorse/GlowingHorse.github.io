---
title: "Channel Attribution Visualization"
layout: single-portfolio
excerpt: "<img src='/images/research/ClsDisVis/cls-dis-teaser.jpg' alt=''>"
collection: research
order_number: 20
header: 
  og_image: "research/ClsDisVis/cls-dis-teaser.jpg"
---

To understand how CNNs work, it is necessary to evaluate what features extracted by convolutional kernels affect the particular output, because the contributions of different features to one output are often noticeably different. To achieve this goal, we define a concept, *i.e.,* **class-discriminative feature groups**, to specify features corresponding to the particular output class of a network that are extracted by groups of convolutional kernels. Then, we design an attribution-based loss function to generate class-discriminative feature visualizations. The visualization results uncover a new and unique facet of visual interpretability such that the same image region may contribute to different outputs with different feature implications.

![](/images/research/ClsDisVis/cls-dis-top.jpg){: .align-center .fifty-percentage-resize-img}

## Article

"Group visualization of class-discriminative features." *Neural Networks*. [Article](https://www.sciencedirect.com/science/article/pii/S0893608020301969){: .btn} [GitHub Repo](https://github.com/GlowingHorse/Class-Discriminative-Vis){: .btn}
